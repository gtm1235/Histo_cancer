{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a custom CNN for detection of cancer from histology specimens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##first imports for torch, torchvision, numpy, pandas, and os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import PIL\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n"
     ]
    }
   ],
   "source": [
    "#Set up for cuda usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "print(\"{} is available\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I seprate the training set into sperate train and validation folders -- I left it up to the user to place directories and creat the 0 and 1 subfolders because of different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganizing data to subfolders zero and one for pytorch labels implementation as well as seperate into valid folders sperately \n",
    "# to seperate image augmentation transforms and validation transforms\n",
    "\n",
    "labels = pd.read_csv(r'''C:\\Users\\gtm12\\OneDrive\\Documents\\Jupyter Notebooks\\Kaggle\\Histo_cancer\\train_labels.csv''')\n",
    "\n",
    "training_path = r'''C:\\Users\\gtm12\\OneDrive\\Documents\\Jupyter Notebooks\\Kaggle\\Histo_cancer\\histo_data\\train\\\\'''\n",
    "train_path_0 = r'''C:\\Users\\gtm12\\OneDrive\\Documents\\Jupyter Notebooks\\Kaggle\\Histo_cancer\\histo_data\\train\\0\\\\'''\n",
    "train_path_1 = r'''C:\\Users\\gtm12\\OneDrive\\Documents\\Jupyter Notebooks\\Kaggle\\Histo_cancer\\histo_data\\train\\1\\\\'''\n",
    "valid_path_0 = r'''C:\\Users\\gtm12\\OneDrive\\Documents\\Jupyter Notebooks\\Kaggle\\Histo_cancer\\histo_data\\valid\\0\\\\'''\n",
    "valid_path_1 = r'''C:\\Users\\gtm12\\OneDrive\\Documents\\Jupyter Notebooks\\Kaggle\\Histo_cancer\\histo_data\\valid\\1\\\\'''\n",
    "\n",
    "percent_valid = .15\n",
    "\n",
    "\n",
    "def file_valid_split_copy_delete(training_path, train_path_0, train_path_1, valid_path_0, valid_path_1, percent_valid)\n",
    "    counter_0 = 0\n",
    "    counter_1 = 0\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        if labels.label[i] == 0:\n",
    "            counter_false +=1\n",
    "        if labels.label[i] == 1:\n",
    "            counter_true +=1\n",
    "\n",
    "    train_split_0 = int(percent_valid * counter_false)\n",
    "    train_split_1 = int(percent_valid * counter_true)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        file_name = str(labels.id[i]) + '.tif'\n",
    "        if labels.label[i].item() == 0:\n",
    "            if counter_0 <= train_split_0:\n",
    "                file_path_copy = valid_path_0\n",
    "                counter_0 += 1\n",
    "                file_origin = training_path + file_name\n",
    "                file_path_copy = file_path_copy + file_name\n",
    "                shutil.copy2(file_origin, file_path_copy)\n",
    "                os.remove(file_origin)\n",
    "            else:\n",
    "                file_path_copy = train_path_0\n",
    "                counter_0 += 1\n",
    "                file_origin = training_path + file_name\n",
    "                file_path_copy = file_path_copy + file_name\n",
    "                shutil.copy2(file_origin, file_path_copy)\n",
    "                os.remove(file_origin)\n",
    "\n",
    "        elif labels.label[i].item() == 1:\n",
    "            if counter_1 <= trains_split_1:\n",
    "                file_path_copy = valid_path_1\n",
    "                counter_1 += 1\n",
    "                file_origin = training_path + file_name\n",
    "                file_path_copy = file_path_copy + file_name\n",
    "                shutil.copy2(file_origin, file_path_copy)\n",
    "                os.remove(file_origin)\n",
    "            else:\n",
    "                file_path_copy = train_path_1\n",
    "                counter_0 += 1\n",
    "                file_origin = training_path + file_name\n",
    "                file_path_copy = file_path_copy + file_name\n",
    "                shutil.copy2(file_origin, file_path_copy)\n",
    "                os.remove(file_origin)\n",
    "                \n",
    "    print(\"files have been relocated\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_valid_split_copy_delete(training_path, train_path_0, train_path_1, valid_path_0, valid_path_1, percent_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin to load Data Transforms and split validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 1000\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.15\n",
    "#file directories\n",
    "data_dir =  r'''C:\\Users\\gtm12\\OneDrive\\Documents\\Jupyter Notebooks\\Kaggle\\Histo_cancer\\histo_data\\\\'''\n",
    "train_dir = data_dir + 'train\\\\'\n",
    "valid_dir = data_dir + 'valid\\\\'\n",
    "test_dir = data_dir + 'test\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your transforms for the training and validation sets\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(15),\n",
    "                                      transforms.RandomResizedCrop(32),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ColorJitter(brightness=0.4,\n",
    "                                                             contrast=0.4,\n",
    "                                                             saturation = 0.4,\n",
    "                                                             hue=0.1),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                          [0.229, 0.224, 0.225])])\n",
    "\n",
    "valid_transforms = transforms.Compose([transforms.Resize(48),\n",
    "                                      transforms.CenterCrop(32),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                          [0.229, 0.224, 0.225])])\n",
    "\n",
    "#test_transforms = transforms.Compose([transforms.CenterCrop(32),\n",
    "#                                      transforms.ToTensor(),\n",
    "#                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "#                                                          [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "train_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "valid_data = datasets.ImageFolder(valid_dir, transform=valid_transforms)\n",
    "#test_data = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
    "\n",
    "\n",
    "# TODO: Using the image datasets and the trainforms, define the dataloaders\n",
    "#trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "#validloader = torch.utils.data.DataLoader(valid_data, batch_size=64, shuffle=True)\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    num_workers=num_workers, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, \n",
    "    num_workers=num_workers, shuffle=True)\n",
    "#test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "#    num_workers=num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to un-normalize and display an image\n",
    "def imshow(img):\n",
    "    img = .224*img + .456  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-03df42e14a7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# convert images to numpy for display\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# plot the images in the batch, along with the corresponding labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# display 20 images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "# display 20 images\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(labels[idx].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(valid_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "# display 20 images\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(labels[idx].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Time to define our CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 3)\n",
    "        self.conv3 = nn.Conv2d(12, 16, 2)\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        #FC Layers\n",
    "        self.hidden_1 = nn.Linear(64, 64)\n",
    "        self.hidden_2 = nn.Linear(64, 32)\n",
    "        self.hidden_3 = nn.Linear(32, 2)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "#        print(x.shape)\n",
    "        x = F.relu(self.hidden_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.hidden_2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.log_softmax(self.hidden_3(x), dim=1)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "\n",
    "# create a complete CNN\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Add Optimizer and Loss finctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=.00075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 25 # you may increase this number to train a final model\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "           \n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'histo_cancer_model.pth')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
